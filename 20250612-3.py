import streamlit as st
import random # å°å…¥ random æ¨¡çµ„
# LangChain ç›¸é—œçš„å°å…¥
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama

# region --- Streamlit è¨­å®š ---
st.set_page_config(page_title="æ™ºèƒ½åŠ©ç†", layout="centered", initial_sidebar_state="auto")
st.title("æ™ºèƒ½åŠ©ç†")
st.markdown("---")

# ç”Ÿæˆ 0.0 åˆ° 1.0 ä¹‹é–“çš„éš¨æ©Ÿæº«åº¦å€¼
random_initial_temperature = round(random.uniform(0.0, 1.0), 2) # å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œå…©ä½ï¼Œä»¥ä¾¿æ»‘æ¡¿é¡¯ç¤º

temperature = st.slider(
    "AI temperature (æ•¸å€¼è¶Šå¤§ï¼ŒAIæä¾›è¶Šç†±æƒ…çš„æœå‹™æ…‹åº¦)",
    max_value=1.0,
    min_value=0.0,
    step=0.01,
    value=random_initial_temperature # ä½¿ç”¨éš¨æ©Ÿç”Ÿæˆçš„åˆå§‹å€¼
)
# é è¨­çš„çŸ¥è­˜åº«æ–‡æœ¬ç¯„ä¾‹ (æ–¹ä¾¿ä½¿ç”¨è€…åƒè€ƒ)
ph = """æœ¬å…¬å¸å°ˆé–€éŠ·å”®é«˜å“è³ªçš„æœ‰æ©Ÿå’–å•¡è±†, æˆ‘å€‘åªå¾å·´è¥¿ã€å“¥å€«æ¯”äºå’Œè¡£ç´¢æ¯”äºçš„å…¬å¹³è²¿æ˜“è¾²å ´é€²å£å’–å•¡è±†.æˆ‘å€‘çš„çƒ˜ç„™å¸«å‚…æ“æœ‰è¶…é20å¹´çš„ç¶“é©—, ç¢ºä¿æ¯ä¸€æ‰¹å’–å•¡è±†éƒ½èƒ½å®Œç¾çƒ˜ç„™, å¸¶å‡ºå…¶ç¨ç‰¹çš„é¢¨å‘³.é¡§å®¢æœå‹™æ˜¯æˆ‘å€‘çš„é¦–è¦ä»»å‹™, æ‰€æœ‰è¨‚å–®åœ¨24å°æ™‚å…§å‡ºè²¨, ä¸¦æä¾›7å¤©å…§ç„¡æ¢ä»¶é€€è²¨æœå‹™.ç›®å‰æˆ‘å€‘çš„ç†±éŠ·ç”¢å“æœ‰ï¼šå·´è¥¿é™½å…‰å’–å•¡è±† (ä¸­åº¦çƒ˜ç„™, å¸¶æœ‰å …æœé¦™æ°£). å“¥å€«æ¯”äºé»ƒé‡‘å’–å•¡è±† (æ·±åº¦çƒ˜ç„™,å£æ„Ÿæ¿ƒéƒ) å’Œè¡£ç´¢æ¯”äºèŠ±åœ’å’–å•¡è±† (æ·ºåº¦çƒ˜ç„™ï¼Œå¸¶æœ‰èŠ±æœé¦™)ã€‚
        æˆ‘å€‘ä¹Ÿæä¾›å’–å•¡å™¨å…·å’Œå’–å•¡ç ”ç£¨æ©Ÿçš„éŠ·å”®ï¼Œä¸¦ä¸”å®šæœŸèˆ‰è¾¦å’–å•¡å“é‘‘æœƒå’Œçƒ˜ç„™èª²ç¨‹.æ‰€æœ‰ç”¢å“çš„éŠ·å”®åˆ©æ½¤çš„5%å°‡æè´ˆçµ¦å…¨çƒçš„å’–å•¡è¾²æ‰¶åŠ©åŸºé‡‘.æˆ‘å€‘æ¥å—Visaã€MasterCardå’ŒAmexä¿¡ç”¨å¡æ”¯ä»˜.å°æ–¼æ‰¹ç™¼è¨‚å–®, æˆ‘å€‘æä¾›ç‰¹æ®Šçš„æŠ˜æ‰£.å…¬å¸ç¸½éƒ¨ä½æ–¼å°åŒ—å¸‚ï¼Œæˆ‘å€‘çš„å®¢æˆ¶æœå‹™ç†±ç·šæ˜¯ (02) 1234-5678ã€‚"""

# è®“ä½¿ç”¨è€…è¼¸å…¥çŸ¥è­˜åº«æ–‡æœ¬
user_context_text = st.text_area(
    label="è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ‚¨çš„è³‡è¨Šä½œç‚ºçŸ¥è­˜åº«ï¼š",
    value="",  # åˆå§‹å€¼ç‚ºç©º
    height=300,
    placeholder="ä»¥ä¸€é–“å’–å•¡éŠ·å”®å…¬å¸ç‚ºç¯„ä¾‹:\n" + ph  # é¡¯ç¤ºç¯„ä¾‹æç¤º
)

# åˆ¤æ–·æœ€çµ‚ä½¿ç”¨çš„çŸ¥è­˜åº«æ–‡æœ¬ï¼šå¦‚æœä½¿ç”¨è€…è¼¸å…¥ç‚ºç©ºï¼Œå‰‡ä½¿ç”¨é è¨­ç¯„ä¾‹æ–‡æœ¬
final_context_text = ph if len(user_context_text.strip()) == 0 else user_context_text.strip()


# ç”¨æ–¼è¨­å®š RAG ç³»çµ±çš„å‡½æ•¸ï¼ˆä¸å†ç·©å­˜ï¼Œå› ç‚ºçŸ¥è­˜åº«æ˜¯å‹•æ…‹çš„ï¼‰
def setup_rag_system_dynamic(context_text, temp):
    """ è¨­å®šä¸¦è¿”å› RAG ç³»çµ±çµ„ä»¶ (å‘é‡è³‡æ–™åº«, æª¢ç´¢å™¨, LLM, RAGéˆ)ã€‚è©²å‡½æ•¸æœƒæ ¹æ“šå‚³å…¥çš„ context_text å»ºç«‹æ–°çš„çŸ¥è­˜åº«ã€‚"""
    knowledge_file = "company_knowledge_base.txt"  # 0. å°‡æ–‡æœ¬å¯«å…¥ä¸€å€‹è‡¨æ™‚æ–‡ä»¶ï¼Œä»¥ä¾¿ TextLoader è®€å–
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write(context_text)
    loader = TextLoader(knowledge_file, encoding="utf-8")  # 1. è¼‰å…¥ä¸¦åˆ†å‰²æ–‡æœ¬
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)

    # 2. ç”ŸæˆåµŒå…¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
    embeddings = OllamaEmbeddings(model="nomic-embed-text")  # è«‹ç¢ºä¿ä½ å·²ç¶“ç”¨ 'ollama pull nomic-embed-text' ä¸‹è¼‰äº†åµŒå…¥æ¨¡å‹
    vectorstore = Chroma.from_documents(chunks, embeddings)  # å»ºç«‹ Chroma å‘é‡è³‡æ–™åº«ã€‚æ¯æ¬¡èª¿ç”¨éƒ½æœƒå‰µå»ºä¸€å€‹æ–°çš„ã€ç¨ç«‹çš„ in-memory è³‡æ–™åº«ã€‚
    retriever = vectorstore.as_retriever()

    # 3. å®šç¾©å¤šå€‹ LLM å’Œå…¶å°æ‡‰çš„ RAG éˆ
    # æ³¨æ„ï¼šé€™è£¡å®šç¾©æ¨¡å‹è™•ç†çš„é †åºï¼Œå°‡æœƒåœ¨å¾Œé¢éš¨æ©Ÿæ‰“äº‚
    models_config = [
        ("llama3.2", ChatOllama(model="llama3.2", temperature=temp)),
        ("gemma3:4b", ChatOllama(model="gemma3:4b", temperature=temp)),
        ("phi4", ChatOllama(model="phi4", temperature=temp))
    ]

    # éš¨æ©Ÿæ‰“äº‚æ¨¡å‹é †åº
    random.shuffle(models_config)
    model_order = [name for name, _ in models_config]

    rag_chains = {}
    
    for model_name, llm_instance in models_config: # ä½¿ç”¨éš¨æ©Ÿæ‰“äº‚å¾Œçš„é †åº
        # æç¤ºæ¨¡æ¿ç¾åœ¨éœ€è¦èƒ½å¤ æ¥æ”¶ã€Œå‰ä¸€å€‹æ¨¡å‹çš„å›ç­”ã€
        # æˆ‘å€‘å°‡ä½¿ç”¨ "previous_answer" ä½œç‚ºé¡å¤–çš„ä¸Šä¸‹æ–‡è®Šæ•¸
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©ç†ï¼Œç¾åœ¨ä½ ä½¿ç”¨çš„æ¨¡å‹æ˜¯ {model_name}ã€‚
              è«‹æ ¹æ“šä»¥ä¸‹æª¢ç´¢åˆ°çš„è³‡è¨Šä¾†å›ç­”å•é¡Œã€‚
              
              å¦‚æœä½ æ˜¯ç¬¬äºŒå€‹æˆ–ç¬¬ä¸‰å€‹å›ç­”çš„æ¨¡å‹ï¼Œä½ å¯ä»¥åƒè€ƒã€Œå‰ä¸€å€‹æ¨¡å‹å›ç­”çš„å…§å®¹ã€ä¾†è£œå……ã€ä¿®æ­£æˆ–æä¾›ä¸åŒè¦–è§’ï¼Œè®“å›ç­”æ›´å®Œæ•´è±å¯Œã€‚
              
              å¦‚æœè³‡è¨Šä¸­æ²’æœ‰æåˆ°ï¼Œè«‹ç¦®è²Œåœ°èªªä½ ç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚
              
              \n\næª¢ç´¢åˆ°çš„è³‡è¨Šï¼š\n{{context}}
              \n\nå‰ä¸€å€‹æ¨¡å‹å›ç­”çš„å…§å®¹ï¼š\n{{previous_answer}}
              """),
            ("human", "{question}")
        ])
        
        # input_mapper ç¾åœ¨éœ€è¦åŒ…å« previous_answerï¼Œå³ä½¿å®ƒæ˜¯ç©ºçš„
        rag_chain_input_mapper = {
            "context": retriever,
            "question": RunnablePassthrough(),
            "previous_answer": RunnablePassthrough() # é€™è£¡éœ€è¦ä¸€å€‹ä½”ä½ç¬¦ï¼Œå¯¦éš›å€¼åœ¨ invoke æ™‚å‚³å…¥
        }
        rag_chains[model_name] = rag_chain_input_mapper | rag_prompt | llm_instance | StrOutputParser()

    return rag_chains, model_order # è¿”å› RAG éˆå­—å…¸å’Œæ¨¡å‹çš„éš¨æ©Ÿé †åº

# ç”±æ–¼ä¸å†ç”Ÿæˆé—œéµå­—ï¼Œæ­¤å‡½æ•¸å°‡ä¸å†è¢«ä½¿ç”¨ï¼Œä½†ç‚ºäº†ç¨‹å¼ç¢¼å®Œæ•´æ€§æš«æ™‚ä¿ç•™
def highlight_important_tokens(text, keywords):
    """
    ç”¨ç´…è‰²ç²—é«”æ¨™è¨˜æ–‡æœ¬ä¸­çš„é—œéµå­—ã€‚
    keywords æ‡‰è©²æ˜¯ä¸€å€‹åˆ—è¡¨æˆ–é›†åˆï¼ŒåŒ…å«è¦æ¨™è¨˜çš„é—œéµå­—ã€‚
    """
    highlighted_text = text
    # Split keywords string into a list
    keyword_list = [k.strip() for k in keywords.split(',') if k.strip()]

    for keyword in keyword_list:
        # Using a simple replace. For more robust highlighting (e.g., avoiding partial word matches),
        # you might need regex with word boundaries.
        highlighted_text = highlighted_text.replace(
            keyword, f'<span style="color:red;font-weight:bold;">{keyword}</span>'
        )
    return highlighted_text


# endregion

# region --- åˆå§‹è®Šæ•¸ ---
if "messages" not in st.session_state:  # æ­·å²è¨Šæ¯
    st.session_state.messages = []
if "rag_system_ready" not in st.session_state:  # ç•¶ç³»çµ±æº–å‚™å¥½çš„æ™‚å€™ï¼Œä¹Ÿå°±æ˜¯ä»£è¡¨ä½¿ç”¨è€…å·²ç¶“ç¢ºèªå°‡è³‡æ–™åšåˆå§‹åŒ–ï¼Œæ¥è‘—æ–¹èƒ½é–‹å•Ÿå•ç­”
    st.session_state.rag_system_ready = False
if "rag_chains" not in st.session_state:  # ragéŠæ ¸å¿ƒï¼Œæˆ‘å€‘æ˜¯åˆ©ç”¨ rag_chains[model_name].invoke() ä¾†å–å¾— ai è³‡è¨Š
    st.session_state.rag_chains = {}
if "model_order" not in st.session_state: # å„²å­˜æ¨¡å‹çš„å›ç­”é †åº
    st.session_state.model_order = []
if "initial_kb_loaded" not in st.session_state:  # è¿½è¹¤æ˜¯å¦å·²é¦–æ¬¡è¼‰å…¥çŸ¥è­˜åº«
    st.session_state.initial_kb_loaded = False
if "show_confirm_modal" not in st.session_state:  # æ˜¯å¦é¡¯ç¤ºå†ç¢ºèªæ–¹æ¡†
    st.session_state.show_confirm_modal = False
if "meta_data_by_model" not in st.session_state: # å„²å­˜æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•¸æ“š (ç¾åœ¨å°‡ä¸å†ä½¿ç”¨)
    st.session_state.meta_data_by_model = {}
# endregion

# region --- åˆå§‹æŒ‰éˆ• ---
if st.button("åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«"):  # ç•¶ä½¿ç”¨è€…é»æ“Šæ­¤æŒ‰éˆ•æ™‚ï¼Œæ ¹æ“šç‹€æ…‹æ±ºå®šæ˜¯ç›´æ¥åˆå§‹åŒ–é‚„æ˜¯é¡¯ç¤ºç¢ºèªæ–¹æ¡†
    if not st.session_state.initial_kb_loaded:  # ç¬¬ä¸€æ¬¡é»æ“Šï¼šç›´æ¥åˆå§‹åŒ–çŸ¥è­˜åº«
        if len(final_context_text) == 0:
            st.error("è«‹åœ¨æ–‡å­—å€åŸŸä¸­è¼¸å…¥æ‚¨çš„è³‡è¨Šï¼Œæˆ–ä½¿ç”¨ç¯„ä¾‹æ–‡æœ¬ã€‚")
        else:
            with st.spinner("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹ï¼Œè«‹ç¨å€™..."):
                try:
                    rag_chains, model_order = setup_rag_system_dynamic(final_context_text, temperature)
                    st.session_state.rag_chains = rag_chains
                    st.session_state.model_order = model_order # å„²å­˜æ¨¡å‹é †åº
                    st.session_state.meta_data_by_model = {} # ç¢ºä¿æ¸…ç©ºæˆ–ä¿æŒç‚ºç©º
                    st.session_state.rag_system_ready = True
                    st.session_state.initial_kb_loaded = True  # æ¨™è¨˜ç‚ºå·²è¼‰å…¥
                    st.success("ğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç¾åœ¨å¯ä»¥é–‹å§‹æå•äº†ã€‚")
                    st.session_state.messages = []  # æ¸…ç©ºèŠå¤©æ­·å²
                except Exception as e:
                    st.session_state.rag_system_ready = False
                    st.session_state.rag_chains = {}
                    st.session_state.model_order = []
                    st.session_state.meta_data_by_model = {} # æ¸…ç©ºå…ƒæ•¸æ“š
                    st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ‰€éœ€æ¨¡å‹å·²ä¸‹è¼‰ã€‚")
                    st.warning("è«‹ç¢ºä¿æ‚¨å·²å®‰è£ Ollama ä¸¦å·²æ‹‰å–ä»¥ä¸‹æ‰€æœ‰æ¨¡å‹ï¼š`nomic-embed-text`ã€`llama3.2`ã€`gemma3:4b` å’Œ `phi4`ã€‚")
                    st.warning("â— **é‡è¦æç¤ºï¼š** å„˜ç®¡ `gemma3:4b` æ”¯æ´å¤šæ¨¡æ…‹ï¼Œä½†æ‚¨çš„æ‡‰ç”¨ç›®å‰åƒ…æ”¯æ´æ–‡æœ¬RAGã€‚")
    else:
        st.session_state.show_confirm_modal = True  # å¦‚æœå·²ç¶“åˆå§‹åŒ–éï¼Œå‰‡è·³åˆ° show_confirm_modal ç¢ºèªæ–¹æ¡†é‚è¼¯(ä¸‹æ–¹)
        st.rerun()  # å¼·åˆ¶ Streamlit é‡æ–°åŸ·è¡Œï¼Œä»¥ç«‹å³é¡¯ç¤ºç¢ºèªæ–¹æ¡†
# endregion

# region --- æ–¹æ¡†é‚è¼¯ ---
if st.session_state.show_confirm_modal:  # åªæœ‰ç•¶ st.session_state.show_confirm_modal ç‚º True æ™‚æ‰é¡¯ç¤º
    st.markdown("---")  # åˆ†éš”ç·š
    st.info("â„¹ï¸ æ‚¨å·²ç¶“åˆå§‹åŒ–éçŸ¥è­˜åº«ã€‚å†æ¬¡é»æ“Šè¡¨ç¤ºæ‚¨è¦**æ›´æ–°**çŸ¥è­˜åº«ã€‚")
    st.write("æ˜¯å¦ç¢ºå®šè¦ç”¨ç•¶å‰æ–‡å­—å€åŸŸçš„è³‡è¨Šä¾†æ›´æ–°çŸ¥è­˜åº«ï¼Ÿé€™å°‡æ¸…é™¤ç•¶å‰å°è©±æ­·å²ï¼Œä¸¦ä¸”ç¢ºèªæ‚¨æä¾›çš„è³‡è¨Š**æ˜¯å¦å‰å¾Œæœ‰ç‰´è§¸***ã€‚")
    col_confirm, col_cancel = st.columns(2)  # è¨­ç½®å…©å€‹æŒ‰éˆ•ï¼Œç”¨æ–¼ç¢ºèªæˆ–å–æ¶ˆæ“ä½œ
    with col_confirm:
        if st.button("ç¢ºå®šæ›´æ–°", key="confirm_update_kb_button"):  # é»æ“Šã€Œç¢ºå®šæ›´æ–°ã€æŒ‰éˆ•
            if len(final_context_text) == 0:  # åŸ·è¡ŒçŸ¥è­˜åº«æ›´æ–°çš„é‚è¼¯
                st.error("è«‹åœ¨æ–‡å­—å€åŸŸä¸­è¼¸å…¥æ‚¨çš„è³‡è¨Šï¼Œæˆ–ä½¿ç”¨ç¯„ä¾‹æ–‡æœ¬ã€‚")
                st.session_state.show_confirm_modal = False  # å‡ºéŒ¯æ™‚éš±è—æ–¹æ¡†
                st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
            else:
                with st.spinner("ğŸ”„ æ­£åœ¨æ›´æ–°çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹ï¼Œè«‹ç¨å€™..."):
                    try:
                        rag_chains, model_order = setup_rag_system_dynamic(final_context_text, temperature)
                        st.session_state.rag_chains = rag_chains
                        st.session_state.model_order = model_order # å„²å­˜æ¨¡å‹é †åº
                        st.session_state.meta_data_by_model = {} # ç¢ºä¿æ¸…ç©ºæˆ–ä¿æŒç‚ºç©º
                        st.session_state.rag_system_ready = True
                        st.success("âœ… çŸ¥è­˜åº«å·²æˆåŠŸæ›´æ–°ï¼")
                        st.session_state.messages = []  # æ¸…ç©ºèŠå¤©æ­·å²
                        st.session_state.show_confirm_modal = False  # éš±è—ç¢ºèªæ–¹æ¡†
                        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
                    except Exception as e:
                        st.session_state.rag_system_ready = False
                        st.session_state.rag_chains = {}
                        st.session_state.model_order = []
                        st.session_state.meta_data_by_model = {} # æ¸…ç©ºå…ƒæ•¸æ“š
                        st.error(f"æ›´æ–°å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ‰€éœ€æ¨¡å‹å·²ä¸‹è¼‰ã€‚")
                        st.session_state.show_confirm_modal = False  # å‡ºéŒ¯æ™‚éš±è—æ–¹æ¡†
                        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
    with col_cancel:
        if st.button("å–æ¶ˆ", key="cancel_update_kb_button"):  # é»æ“Šã€Œå–æ¶ˆã€æŒ‰éˆ•
            st.session_state.show_confirm_modal = False  # éš±è—ç¢ºèªæ–¹æ¡†
            st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
# endregion

# region --- é¡¯ç¤ºæ‘˜è¦å’Œé—œéµå­— (æ­¤å€å¡Šå·²ç§»é™¤) ---
# ç”±æ–¼å…ƒæ•¸æ“šçš„ç”Ÿæˆå’Œé¡¯ç¤ºåŠŸèƒ½å·²ç§»é™¤ï¼Œæ­¤å€å¡Šçš„å…§å®¹ä¸å†éœ€è¦
# endregion

# region --- èŠå¤©ä»‹é¢é‚è¼¯ ---
# é¡¯ç¤ºæ‰€æœ‰æ­·å²è¨Šæ¯ï¼Œå€åˆ†ç”¨æˆ¶å’Œä¸åŒæ¨¡å‹çš„åŠ©ç†å›è¦†
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user"):
            st.markdown(message["content"])
    elif message["role"] == "assistant":
        with st.chat_message("assistant"):
            # å¦‚æœæ˜¯å¤šæ¨¡å‹å›è¦†ï¼Œå‰‡é¡¯ç¤ºæ¨¡å‹åç¨±
            if "model" in message:
                st.markdown(f"**ğŸ¤– {message['model']} çš„å›è¦†ï¼š**")
            st.markdown(message["content"])

if st.session_state.rag_system_ready:  # ç²å–ç”¨æˆ¶è¼¸å…¥ (åªæœ‰ç•¶ RAG ç³»çµ±å°±ç·’æ™‚æ‰å•Ÿç”¨è¼¸å…¥æ¡†)
    if not st.session_state.messages:  # åˆ¤æ–·æ˜¯å¦é¡¯ç¤ºé¦–æ¬¡å°è©±çš„åŠ©ç†æç¤ºè¨Šæ¯ (åƒ…åœ¨èŠå¤©æ­·å²ç‚ºç©ºä¸”åŠ©ç†å·²æº–å‚™å¥½æ™‚é¡¯ç¤º)
        initial_chat_message = "æ‚¨å¥½ï¼Œè«‹å•æœ‰ä»€éº¼æˆ‘å¯ä»¥å”åŠ©æ‚¨çš„ï¼Ÿ"
        with st.chat_message("assistant"):
            st.markdown(initial_chat_message)
            st.session_state.messages.append({"role": "assistant", "content": initial_chat_message})
    if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ..."):  # å°‡ç”¨æˆ¶è¨Šæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²ä¸¦é¡¯ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        previous_model_answer = "" # åˆå§‹åŒ–å‰ä¸€å€‹æ¨¡å‹çš„å›ç­”
        
        # è¿­ä»£æ‰€æœ‰ RAG éˆä¸¦ç²å–å›è¦†ï¼Œä¾åºå‚³éå‰ä¸€å€‹æ¨¡å‹çš„å›ç­”
        # ä½¿ç”¨éš¨æ©Ÿæ‰“äº‚å¾Œçš„é †åº st.session_state.model_order
        for model_name in st.session_state.model_order:
            rag_chain = st.session_state.rag_chains[model_name]
            with st.chat_message("assistant"):
                try:
                    st.markdown(f"**ğŸ¤– {model_name} æ­£åœ¨æ€è€ƒä¸­...**") # é¡¯ç¤ºæ¨¡å‹æ­£åœ¨è™•ç†çš„æç¤º
                    
                    # èª¿ç”¨ RAG éˆæ™‚ï¼Œå‚³é prompt å’Œ previous_model_answer
                    ai_response = rag_chain.invoke({
                        "question": prompt,
                        "previous_answer": previous_model_answer
                    })
                    
                    st.markdown(f"**ğŸ¤– {model_name} çš„å›è¦†ï¼š**") # æœ€çµ‚é¡¯ç¤ºæ¨¡å‹åç¨±
                    st.markdown(ai_response)  # é¡¯ç¤ºæœ€çµ‚å›è¦†
                    
                    # å°‡ç•¶å‰æ¨¡å‹çš„å›ç­”å„²å­˜ç‚ºä¸‹ä¸€å€‹æ¨¡å‹çš„ã€Œå‰ä¸€å€‹å›ç­”ã€
                    previous_model_answer = ai_response
                    
                    # å°‡å›è¦†æ·»åŠ åˆ°èŠå¤©æ­·å²ï¼ŒåŒ…å«æ¨¡å‹åç¨±
                    st.session_state.messages.append({"role": "assistant", "model": model_name, "content": ai_response})
                except Exception as e:
                    error_message = f"å¾ˆæŠ±æ­‰ï¼Œ**{model_name}** è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}ã€‚\nè«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹å·²ä¸‹è¼‰ã€‚"
                    st.error(error_message)  # å°‡éŒ¯èª¤è¨Šæ¯ä½œç‚º AI å›è¦†
                    # å°‡éŒ¯èª¤è¨Šæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²
                    st.session_state.messages.append({"role": "assistant", "model": model_name, "content": error_message})
else:  # å¦‚æœ RAG ç³»çµ±å°šæœªå°±ç·’ï¼Œé¡¯ç¤ºæç¤ºè¨Šæ¯ä¸¦ç¦ç”¨èŠå¤©è¼¸å…¥æ¡†
    st.warning("è«‹å…ˆè¼¸å…¥æ‚¨çš„è³‡è¨Šä¸¦é»æ“Šã€Œåˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«ã€æŒ‰éˆ•ä¾†å•Ÿå‹•åŠ©ç†ã€‚")
    st.chat_input("è«‹å…ˆåˆå§‹åŒ–çŸ¥è­˜åº«...", disabled=True)


# endregion

# region --- å´æ¬„èªªæ˜ ---
st.sidebar.markdown("---")
st.sidebar.header("ä½¿ç”¨èªªæ˜")
st.sidebar.markdown("""
é€™å€‹æ‡‰ç”¨ç¨‹å¼æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©ç†ï¼Œå®ƒæœƒ**å®Œå…¨æ ¹æ“šæ‚¨åœ¨æ–‡å­—å€åŸŸä¸­æä¾›çš„æœ€æ–°è³‡è¨Š**ä¾†å›ç­”å•é¡Œã€‚

**æ“ä½œæ­¥é©Ÿï¼š**
1.  åœ¨æ–‡å­—å€åŸŸä¸­**è¼¸å…¥æˆ–ä¿®æ”¹**æ‚¨çš„è³‡è¨Šã€‚
2.  **é‡è¦ï¼**
    * **ç¬¬ä¸€æ¬¡**é»æ“Šã€Œ**åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«**ã€æŒ‰éˆ•ï¼Œå°‡æœƒè¼‰å…¥è³‡è¨Šä¸¦å•Ÿå‹•åŠ©ç†ã€‚
    * **å¾ŒçºŒ**é»æ“Šæ­¤æŒ‰éˆ•ï¼Œæœƒå½ˆå‡ºä¸€å€‹ç¢ºèªæ–¹æ¡†ï¼Œè©¢å•æ‚¨æ˜¯å¦è¦**æ›´æ–°**çŸ¥è­˜åº«ã€‚é»æ“Šã€Œç¢ºå®šæ›´æ–°ã€å°‡æœƒæ¸…é™¤èˆŠå°è©±ä¸¦ä½¿ç”¨æ–°è³‡è¨Šã€‚
3.  ç•¶çœ‹åˆ°ã€ŒğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ã€æˆ–ã€Œâœ… çŸ¥è­˜åº«å·²æˆåŠŸæ›´æ–°ï¼ã€è¨Šæ¯å¾Œï¼Œæ‚¨å°±å¯ä»¥åœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†ä¸­æå•äº†ã€‚

**æ¨¡å‹è¡Œç‚ºï¼š**
* ç³»çµ±æœƒä½¿ç”¨ä¸‰å€‹èªè¨€æ¨¡å‹ï¼ˆ`llama3.2`ã€`gemma3:4b`ã€`phi4`ï¼‰ä¾åºå›ç­”æ‚¨çš„å•é¡Œã€‚
* **æ¯æ¬¡åˆå§‹åŒ–æˆ–é‡æ–°åŸ·è¡Œæ‡‰ç”¨ç¨‹å¼æ™‚ï¼Œé€™ä¸‰å€‹æ¨¡å‹çš„å›ç­”é †åºéƒ½æœƒæ˜¯éš¨æ©Ÿçš„ã€‚**
* æ¯å€‹æ¨¡å‹åœ¨å›ç­”æ™‚ï¼Œéƒ½å¯ä»¥**åƒè€ƒå‰ä¸€å€‹æ¨¡å‹çµ¦å‡ºçš„ç­”æ¡ˆ**ï¼Œé€™æœ‰åŠ©æ–¼ç­”æ¡ˆçš„è£œå……ã€ä¿®æ­£æˆ–æä¾›ä¸åŒè¦–è§’ã€‚

**é‡è¦æé†’ï¼š**
è«‹ç¢ºä¿æ‚¨çš„é›»è…¦å·²**å®‰è£ Ollama æœå‹™**ä¸¦å·²**ä¸‹è¼‰ä»¥ä¸‹æ‰€æœ‰æ¨¡å‹**ï¼š
-   **åµŒå…¥æ¨¡å‹ (Embedding Model):** `nomic-embed-text`
    åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull nomic-embed-text`
-   **èªè¨€æ¨¡å‹ (Language Model) çµ„åˆï¼š**
    * `llama3.2`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull llama3.2`
    * `gemma3:4b`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull gemma3:4b`
    * `phi4`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull phi4`

**é—œæ–¼æ¨¡å‹èƒ½åŠ›çš„æ³¨æ„äº‹é …ï¼š**
* `gemma3:4b` æ˜¯ä¸€å€‹æ”¯æ´**å¤šæ¨¡æ…‹ (åœ–åƒèˆ‡æ–‡æœ¬)** çš„å„ªç§€æ¨¡å‹ï¼ç„¶è€Œï¼Œåœ¨ç›®å‰é€™å€‹æ‡‰ç”¨ç¨‹å¼ä¸­ï¼Œ**æ‚¨çš„çŸ¥è­˜åº«æ˜¯ç´”æ–‡æœ¬ï¼Œä¸”ç¨‹å¼ç¢¼é‚è¼¯ä¹Ÿåªè™•ç†æ–‡å­—è¼¸å…¥**ã€‚è‹¥è¦å¯¦ç¾åœ–ç‰‡è¼¸å…¥å’ŒåŸºæ–¼åœ–ç‰‡çš„ RAGï¼Œéœ€è¦å°ç¨‹å¼ç¢¼é€²è¡Œé€²ä¸€æ­¥çš„é–‹ç™¼å’Œä¿®æ”¹ã€‚
* å°æ–¼çŸ¥è­˜åº«ä¸­æ²’æœ‰çš„è³‡è¨Šï¼ŒAI æœƒç¦®è²Œåœ°è¡¨ç¤ºç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚
""")
# endregion