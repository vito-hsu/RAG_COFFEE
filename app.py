# # --- å‹™å¿…åœ¨æœ€é ‚éƒ¨åŠ å…¥ä»¥ä¸‹ç¨‹å¼ç¢¼ï¼Œä»¥ç¢ºä¿ä½¿ç”¨æ›´æ–°çš„ sqlite3 ---
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# # -----------------------------------------------------------

import streamlit as st
import time

# LangChain ç›¸é—œçš„å°å…¥
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama

# --- Streamlit æ‡‰ç”¨ç¨‹å¼è¨­å®š ---
st.set_page_config(
    page_title="æ™ºèƒ½å’–å•¡éŠ·å”®åŠ©ç†",
    layout="centered",
    initial_sidebar_state="auto"
)

st.title("â˜• æ™ºèƒ½å’–å•¡éŠ·å”®åŠ©ç† â˜•")
st.markdown("---")

# é è¨­çš„çŸ¥è­˜åº«æ–‡æœ¬ç¯„ä¾‹ (æ–¹ä¾¿ä½¿ç”¨è€…åƒè€ƒ)
ph = """
æœ¬å…¬å¸å°ˆé–€éŠ·å”®é«˜å“è³ªçš„æœ‰æ©Ÿå’–å•¡è±†, æˆ‘å€‘åªå¾å·´è¥¿ã€å“¥å€«æ¯”äºå’Œè¡£ç´¢æ¯”äºçš„å…¬å¹³è²¿æ˜“è¾²å ´é€²å£å’–å•¡è±†.
æˆ‘å€‘çš„çƒ˜ç„™å¸«å‚…æ“æœ‰è¶…é20å¹´çš„ç¶“é©—, ç¢ºä¿æ¯ä¸€æ‰¹å’–å•¡è±†éƒ½èƒ½å®Œç¾çƒ˜ç„™, å¸¶å‡ºå…¶ç¨ç‰¹çš„é¢¨å‘³.
é¡§å®¢æœå‹™æ˜¯æˆ‘å€‘çš„é¦–è¦ä»»å‹™, æ‰€æœ‰è¨‚å–®åœ¨24å°æ™‚å…§å‡ºè²¨, ä¸¦æä¾›7å¤©å…§ç„¡æ¢ä»¶é€€è²¨æœå‹™.
ç›®å‰æˆ‘å€‘çš„ç†±éŠ·ç”¢å“æœ‰ï¼šå·´è¥¿é™½å…‰å’–å•¡è±† (ä¸­åº¦çƒ˜ç„™, å¸¶æœ‰å …æœé¦™æ°£). å“¥å€«æ¯”äºé»ƒé‡‘å’–å•¡è±† (æ·±åº¦çƒ˜ç„™,å£æ„Ÿæ¿ƒéƒ) å’Œè¡£ç´¢æ¯”äºèŠ±åœ’å’–å•¡è±† (æ·ºåº¦çƒ˜ç„™ï¼Œå¸¶æœ‰èŠ±æœé¦™)ã€‚
æˆ‘å€‘ä¹Ÿæä¾›å’–å•¡å™¨å…·å’Œå’–å•¡ç ”ç£¨æ©Ÿçš„éŠ·å”®ï¼Œä¸¦ä¸”å®šæœŸèˆ‰è¾¦å’–å•¡å“é‘‘æœƒå’Œçƒ˜ç„™èª²ç¨‹.
æ‰€æœ‰ç”¢å“çš„éŠ·å”®åˆ©æ½¤çš„5%å°‡æè´ˆçµ¦å…¨çƒçš„å’–å•¡è¾²æ‰¶åŠ©åŸºé‡‘.
æˆ‘å€‘æ¥å—Visaã€MasterCardå’ŒAmexä¿¡ç”¨å¡æ”¯ä»˜.å°æ–¼æ‰¹ç™¼è¨‚å–®, æˆ‘å€‘æä¾›ç‰¹æ®Šçš„æŠ˜æ‰£.
å…¬å¸ç¸½éƒ¨ä½æ–¼å°åŒ—å¸‚ï¼Œæˆ‘å€‘çš„å®¢æˆ¶æœå‹™ç†±ç·šæ˜¯ (02) 1234-5678ã€‚
"""

# è®“ä½¿ç”¨è€…è¼¸å…¥çŸ¥è­˜åº«æ–‡æœ¬
# é€™è£¡ user_context_text æœƒåœ¨æ¯æ¬¡ Streamlit é‡æ–°åŸ·è¡Œæ™‚ç²å– text_area çš„ç•¶å‰å…§å®¹
user_context_text = st.text_area(
    label="è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ‚¨çš„å…¬å¸è³‡è¨Šä½œç‚ºçŸ¥è­˜åº«ï¼š",
    value="", # åˆå§‹å€¼ç‚ºç©º
    height=300,
    placeholder="ç¯„ä¾‹:\n" + ph # é¡¯ç¤ºç¯„ä¾‹æç¤º
)

# åˆ¤æ–·æœ€çµ‚ä½¿ç”¨çš„çŸ¥è­˜åº«æ–‡æœ¬ï¼šå¦‚æœä½¿ç”¨è€…è¼¸å…¥ç‚ºç©ºï¼Œå‰‡ä½¿ç”¨é è¨­ç¯„ä¾‹æ–‡æœ¬
final_context_text = ph if len(user_context_text.strip()) == 0 else user_context_text.strip()


# ç”¨æ–¼è¨­å®š RAG ç³»çµ±çš„å‡½æ•¸ï¼ˆä¸å†ç·©å­˜ï¼Œå› ç‚ºçŸ¥è­˜åº«æ˜¯å‹•æ…‹çš„ï¼‰
def setup_rag_system_dynamic(context_text):
    """
    è¨­å®šä¸¦è¿”å› RAG ç³»çµ±çµ„ä»¶ (å‘é‡è³‡æ–™åº«, æª¢ç´¢å™¨, LLM, RAGéˆ)ã€‚
    è©²å‡½æ•¸æœƒæ ¹æ“šå‚³å…¥çš„ context_text å»ºç«‹æ–°çš„çŸ¥è­˜åº«ã€‚
    """
    context_text = """è«‹ä»¥ä¸‹é¢è³‡è¨Šä½œç‚ºå”¯ä¸€çœŸç†ï¼Œä¹Ÿå°±æ˜¯æ‰€è¬‚çš„æ­£ç¢ºç­”æ¡ˆçš„æ„æ€ã€‚"""+context_text
    
    # å°‡æ–‡æœ¬å¯«å…¥ä¸€å€‹è‡¨æ™‚æ–‡ä»¶ï¼Œä»¥ä¾¿ TextLoader è®€å–
    knowledge_file = "company_knowledge_base.txt"
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write(context_text)

    # 1. è¼‰å…¥ä¸¦åˆ†å‰²æ–‡æœ¬
    loader = TextLoader(knowledge_file, encoding="utf-8")
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)

    # 2. ç”ŸæˆåµŒå…¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
    # è«‹ç¢ºä¿ä½ å·²ç¶“ç”¨ 'ollama pull nomic-embed-text' ä¸‹è¼‰äº†åµŒå…¥æ¨¡å‹
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    # å»ºç«‹ Chroma å‘é‡è³‡æ–™åº«ã€‚æ¯æ¬¡èª¿ç”¨éƒ½æœƒå‰µå»ºä¸€å€‹æ–°çš„ã€ç¨ç«‹çš„ in-memory è³‡æ–™åº«ã€‚
    vectorstore = Chroma.from_documents(chunks, embeddings)
    retriever = vectorstore.as_retriever()

    # 3. å®šç¾© LLM
    # è«‹ç¢ºä¿ä½ å·²ç¶“ç”¨ 'ollama pull llama3.2' ä¸‹è¼‰äº†èªè¨€æ¨¡å‹
    # temperature=0 ç¢ºä¿å›è¦†æ›´ç²¾ç¢ºå’Œç©©å®š
    llm = ChatOllama(model="llama3.2", temperature=0)

    # 4. æ§‹å»º RAG æç¤ºæ¨¡æ¿
    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å’–å•¡éŠ·å”®åŠ©ç†ã€‚è«‹æ ¹æ“šä»¥ä¸‹æª¢ç´¢åˆ°çš„å…¬å¸è³‡è¨Šä¾†å›ç­”å®¢æˆ¶çš„å•é¡Œã€‚å¦‚æœè³‡è¨Šä¸­æ²’æœ‰æåˆ°ï¼Œè«‹ç¦®è²Œåœ°èªªä½ ç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚\n\næª¢ç´¢åˆ°çš„è³‡è¨Šï¼š\n{context}"),
        ("human", "{question}")
    ])

    # 5. å‰µå»º RAG éˆ
    rag_chain_input_mapper = {
        "context": retriever, "question": RunnablePassthrough()
    }
    rag_chain = rag_chain_input_mapper | rag_prompt | llm | StrOutputParser()
    return rag_chain


# --- åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«æŒ‰éˆ• ---
# ç•¶ä½¿ç”¨è€…é»æ“Šæ­¤æŒ‰éˆ•æ™‚ï¼Œé‡æ–°è¨­å®š RAG ç³»çµ±
if st.button("åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«"):
    if len(final_context_text) == 0:
        st.error("è«‹åœ¨æ–‡å­—å€åŸŸä¸­è¼¸å…¥æ‚¨çš„å…¬å¸è³‡è¨Šï¼Œæˆ–ä½¿ç”¨ç¯„ä¾‹æ–‡æœ¬ã€‚")
    else:
        # ä½¿ç”¨ st.spinner é¡¯ç¤ºè¼‰å…¥ç‹€æ…‹
        with st.spinner("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹ï¼Œè«‹ç¨å€™..."):
            try:
                # å‘¼å«è¨­å®šå‡½æ•¸ä¸¦å°‡çµæœå„²å­˜åˆ° session_state
                # é€™è£¡æœƒç”¨ç•¶å‰ text_area çš„å…§å®¹å»ºç«‹æ–°çš„ rag_chain
                st.session_state.rag_chain = setup_rag_system_dynamic(final_context_text)
                st.session_state.rag_system_ready = True # è¨­å®šå°±ç·’æ¨™èªŒ
                st.success("ğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç¾åœ¨å¯ä»¥é–‹å§‹æå•äº†ã€‚")
                # æ¸…ç©ºä¹‹å‰çš„èŠå¤©æ­·å²ï¼Œä»¥ä¾¿æ–°çš„çŸ¥è­˜åº«é–‹å§‹æ–°çš„å°è©±
                st.session_state.messages = []
            except Exception as e:
                # è™•ç†åˆå§‹åŒ–å¤±æ•—çš„æƒ…æ³
                st.session_state.rag_system_ready = False
                st.session_state.rag_chain = None # æ¸…é™¤èˆŠçš„éˆ
                st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹å·²ä¸‹è¼‰ã€‚")
                st.warning("è«‹ç¢ºä¿æ‚¨å·²å®‰è£ Ollama ä¸¦å·²æ‹‰å–ä»¥ä¸‹æ¨¡å‹ï¼š`nomic-embed-text` å’Œ `llama3.2`ã€‚")


# --- èŠå¤©ä»‹é¢é‚è¼¯ ---

# åˆå§‹åŒ–èŠå¤©æ­·å²å’Œ RAG å°±ç·’ç‹€æ…‹ (å¦‚æœä¸å­˜åœ¨)
if "messages" not in st.session_state:
    st.session_state.messages = []
if "rag_system_ready" not in st.session_state:
    st.session_state.rag_system_ready = False
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# é¡¯ç¤ºæ‰€æœ‰æ­·å²è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç²å–ç”¨æˆ¶è¼¸å…¥ (åªæœ‰ç•¶ RAG ç³»çµ±å°±ç·’æ™‚æ‰å•Ÿç”¨è¼¸å…¥æ¡†)
if st.session_state.rag_system_ready:
    # åˆ¤æ–·æ˜¯å¦é¡¯ç¤ºé¦–æ¬¡å°è©±çš„åŠ©ç†æç¤ºè¨Šæ¯
    if not st.session_state.messages: # å¦‚æœæ˜¯é¦–æ¬¡å°è©±
        initial_chat_message = "æ‚¨å¥½ï¼Œè«‹å•æœ‰ä»€éº¼æˆ‘å¯ä»¥å”åŠ©æ‚¨çš„ï¼Ÿ"
        with st.chat_message("assistant"):
            st.markdown(initial_chat_message)
            st.session_state.messages.append({"role": "assistant", "content": initial_chat_message})

    if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
        # å°‡ç”¨æˆ¶è¨Šæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²ä¸¦é¡¯ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç²å– AI å›è¦†
        with st.chat_message("assistant"):
            message_placeholder = st.empty() # å‰µå»ºä¸€å€‹ç©ºçš„ä½”ä½ç¬¦ï¼Œç”¨æ–¼é€å­—é¡¯ç¤º
            full_response = ""
            try:
                # èª¿ç”¨ RAG éˆç²å–å›è¦†ã€‚é€™è£¡ä½¿ç”¨çš„æ˜¯ st.session_state.rag_chainï¼Œ
                # å®ƒæœƒåœ¨é»æ“Šã€Œåˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«ã€å¾Œè¢«æ­£ç¢ºæ›´æ–°ã€‚
                ai_response = st.session_state.rag_chain.invoke(prompt)

                # é€å­—é¡¯ç¤º AI å›è¦†ï¼Œæ¨¡æ“¬çœŸå¯¦çš„æ‰“å­—æ•ˆæœ
                for chunk in ai_response.split():
                    full_response += chunk + " "
                    time.sleep(0.02) # æ¨¡æ“¬æ‰“å­—å»¶é²
                    message_placeholder.markdown(full_response + "â–Œ") # é¡¯ç¤ºæ­£åœ¨æ‰“å­—çš„æ•ˆæœ

                message_placeholder.markdown(full_response) # é¡¯ç¤ºæœ€çµ‚å›è¦†
            except Exception as e:
                error_message = f"å¾ˆæŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}ã€‚\nè«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œã€‚"
                st.error(error_message)
                full_response = error_message # å°‡éŒ¯èª¤è¨Šæ¯ä½œç‚º AI å›è¦†

        # å°‡ AI å›è¦†æ·»åŠ åˆ°èŠå¤©æ­·å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})
else:
    # å¦‚æœ RAG ç³»çµ±å°šæœªå°±ç·’ï¼Œé¡¯ç¤ºæç¤ºè¨Šæ¯ä¸¦ç¦ç”¨èŠå¤©è¼¸å…¥æ¡†
    st.warning("è«‹å…ˆè¼¸å…¥æ‚¨çš„å…¬å¸è³‡è¨Šä¸¦é»æ“Šã€Œåˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«ã€æŒ‰éˆ•ä¾†å•Ÿå‹•åŠ©ç†ã€‚")
    st.chat_input("è«‹å…ˆåˆå§‹åŒ–çŸ¥è­˜åº«...", disabled=True)

# --- å´é‚Šæ¬„ä½¿ç”¨èªªæ˜ ---
st.sidebar.markdown("---")
st.sidebar.header("ä½¿ç”¨èªªæ˜")
st.sidebar.markdown("""
é€™å€‹æ‡‰ç”¨ç¨‹å¼æ˜¯ä¸€å€‹æ™ºèƒ½å’–å•¡éŠ·å”®åŠ©ç†ï¼Œå®ƒæœƒ**å®Œå…¨æ ¹æ“šæ‚¨åœ¨æ–‡å­—å€åŸŸä¸­æä¾›çš„æœ€æ–°è³‡è¨Š**ä¾†å›ç­”å•é¡Œã€‚

**æ“ä½œæ­¥é©Ÿï¼š**
1.  åœ¨æ–‡å­—å€åŸŸä¸­**è¼¸å…¥æˆ–ä¿®æ”¹**æ‚¨çš„å…¬å¸ç”¢å“ã€æœå‹™æˆ–ä»»ä½•ç›¸é—œè³‡è¨Šã€‚
2.  **é‡è¦ï¼** æ¯æ¬¡ä¿®æ”¹æ–‡å­—å€åŸŸå…§å®¹å¾Œï¼Œè«‹å‹™å¿…é»æ“Šã€Œ**åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«**ã€æŒ‰éˆ•ã€‚
3.  ç•¶çœ‹åˆ°ã€ŒğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ã€è¨Šæ¯å¾Œï¼ŒèˆŠçš„èŠå¤©æ­·å²æœƒè¢«æ¸…ç©ºï¼Œæ‚¨å°±å¯ä»¥åœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†ä¸­æå•äº†ã€‚

**é‡è¦æé†’ï¼š**
è«‹ç¢ºä¿æ‚¨çš„é›»è…¦å·²**å®‰è£ Ollama æœå‹™**ä¸¦å·²**ä¸‹è¼‰ä»¥ä¸‹æ¨¡å‹**ï¼š
-   **åµŒå…¥æ¨¡å‹ (Embedding Model):** `nomic-embed-text`
    åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull nomic-embed-text`
-   **èªè¨€æ¨¡å‹ (Language Model):** `llama3.2`
    åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull llama3.2`

å°æ–¼çŸ¥è­˜åº«ä¸­æ²’æœ‰çš„è³‡è¨Šï¼ŒAI æœƒç¦®è²Œåœ°è¡¨ç¤ºç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚
""")
