import streamlit as st
# LangChain ç›¸é—œçš„å°å…¥
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.chat_models import ChatOllama

# region --- Streamlit è¨­å®š ---
st.set_page_config(page_title="æ™ºèƒ½åŠ©ç†", layout="centered", initial_sidebar_state="auto")
st.title("æ™ºèƒ½åŠ©ç†")
st.markdown("---")
temperature = st.slider("AI temperature (æ•¸å€¼è¶Šå¤§ï¼ŒAIæä¾›è¶Šç†±æƒ…çš„æœå‹™æ…‹åº¦)", max_value=1.0, min_value=0.0, step=0.01, value=0.0)
# é è¨­çš„çŸ¥è­˜åº«æ–‡æœ¬ç¯„ä¾‹ (æ–¹ä¾¿ä½¿ç”¨è€…åƒè€ƒ)
ph = """æœ¬å…¬å¸å°ˆé–€éŠ·å”®é«˜å“è³ªçš„æœ‰æ©Ÿå’–å•¡è±†, æˆ‘å€‘åªå¾å·´è¥¿ã€å“¥å€«æ¯”äºå’Œè¡£ç´¢æ¯”äºçš„å…¬å¹³è²¿æ˜“è¾²å ´é€²å£å’–å•¡è±†.æˆ‘å€‘çš„çƒ˜ç„™å¸«å‚…æ“æœ‰è¶…é20å¹´çš„ç¶“é©—, ç¢ºä¿æ¯ä¸€æ‰¹å’–å•¡è±†éƒ½èƒ½å®Œç¾çƒ˜ç„™, å¸¶å‡ºå…¶ç¨ç‰¹çš„é¢¨å‘³.é¡§å®¢æœå‹™æ˜¯æˆ‘å€‘çš„é¦–è¦ä»»å‹™, æ‰€æœ‰è¨‚å–®åœ¨24å°æ™‚å…§å‡ºè²¨, ä¸¦æä¾›7å¤©å…§ç„¡æ¢ä»¶é€€è²¨æœå‹™.ç›®å‰æˆ‘å€‘çš„ç†±éŠ·ç”¢å“æœ‰ï¼šå·´è¥¿é™½å…‰å’–å•¡è±† (ä¸­åº¦çƒ˜ç„™, å¸¶æœ‰å …æœé¦™æ°£). å“¥å€«æ¯”äºé»ƒé‡‘å’–å•¡è±† (æ·±åº¦çƒ˜ç„™,å£æ„Ÿæ¿ƒéƒ) å’Œè¡£ç´¢æ¯”äºèŠ±åœ’å’–å•¡è±† (æ·ºåº¦çƒ˜ç„™ï¼Œå¸¶æœ‰èŠ±æœé¦™)ã€‚
        æˆ‘å€‘ä¹Ÿæä¾›å’–å•¡å™¨å…·å’Œå’–å•¡ç ”ç£¨æ©Ÿçš„éŠ·å”®ï¼Œä¸¦ä¸”å®šæœŸèˆ‰è¾¦å’–å•¡å“é‘‘æœƒå’Œçƒ˜ç„™èª²ç¨‹.æ‰€æœ‰ç”¢å“çš„éŠ·å”®åˆ©æ½¤çš„5%å°‡æè´ˆçµ¦å…¨çƒçš„å’–å•¡è¾²æ‰¶åŠ©åŸºé‡‘.æˆ‘å€‘æ¥å—Visaã€MasterCardå’ŒAmexä¿¡ç”¨å¡æ”¯ä»˜.å°æ–¼æ‰¹ç™¼è¨‚å–®, æˆ‘å€‘æä¾›ç‰¹æ®Šçš„æŠ˜æ‰£.å…¬å¸ç¸½éƒ¨ä½æ–¼å°åŒ—å¸‚ï¼Œæˆ‘å€‘çš„å®¢æˆ¶æœå‹™ç†±ç·šæ˜¯ (02) 1234-5678ã€‚"""

# è®“ä½¿ç”¨è€…è¼¸å…¥çŸ¥è­˜åº«æ–‡æœ¬
user_context_text = st.text_area(
    label="è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ‚¨çš„è³‡è¨Šä½œç‚ºçŸ¥è­˜åº«ï¼š",
    value="",  # åˆå§‹å€¼ç‚ºç©º
    height=300,
    placeholder="ä»¥ä¸€é–“å’–å•¡éŠ·å”®å…¬å¸ç‚ºç¯„ä¾‹:\n" + ph  # é¡¯ç¤ºç¯„ä¾‹æç¤º
)

# åˆ¤æ–·æœ€çµ‚ä½¿ç”¨çš„çŸ¥è­˜åº«æ–‡æœ¬ï¼šå¦‚æœä½¿ç”¨è€…è¼¸å…¥ç‚ºç©ºï¼Œå‰‡ä½¿ç”¨é è¨­ç¯„ä¾‹æ–‡æœ¬
final_context_text = ph if len(user_context_text.strip()) == 0 else user_context_text.strip()


# ç”¨æ–¼è¨­å®š RAG ç³»çµ±çš„å‡½æ•¸ï¼ˆä¸å†ç·©å­˜ï¼Œå› ç‚ºçŸ¥è­˜åº«æ˜¯å‹•æ…‹çš„ï¼‰
def setup_rag_system_dynamic(context_text, temp):
    """ è¨­å®šä¸¦è¿”å› RAG ç³»çµ±çµ„ä»¶ (å‘é‡è³‡æ–™åº«, æª¢ç´¢å™¨, LLM, RAGéˆ)ã€‚è©²å‡½æ•¸æœƒæ ¹æ“šå‚³å…¥çš„ context_text å»ºç«‹æ–°çš„çŸ¥è­˜åº«ã€‚"""
    knowledge_file = "company_knowledge_base.txt"  # 0. å°‡æ–‡æœ¬å¯«å…¥ä¸€å€‹è‡¨æ™‚æ–‡ä»¶ï¼Œä»¥ä¾¿ TextLoader è®€å–
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write(context_text)
    loader = TextLoader(knowledge_file, encoding="utf-8")  # 1. è¼‰å…¥ä¸¦åˆ†å‰²æ–‡æœ¬
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)

    # 2. ç”ŸæˆåµŒå…¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«
    embeddings = OllamaEmbeddings(model="nomic-embed-text")  # è«‹ç¢ºä¿ä½ å·²ç¶“ç”¨ 'ollama pull nomic-embed-text' ä¸‹è¼‰äº†åµŒå…¥æ¨¡å‹
    vectorstore = Chroma.from_documents(chunks, embeddings)  # å»ºç«‹ Chroma å‘é‡è³‡æ–™åº«ã€‚æ¯æ¬¡èª¿ç”¨éƒ½æœƒå‰µå»ºä¸€å€‹æ–°çš„ã€ç¨ç«‹çš„ in-memory è³‡æ–™åº«ã€‚
    retriever = vectorstore.as_retriever()

    # 3. å®šç¾©å¤šå€‹ LLM å’Œå…¶å°æ‡‰çš„ RAG éˆ
    models_to_use = {
        "llama3.2": ChatOllama(model="llama3.2", temperature=temp), # DeepSeek æ›¿æ›ç‚º Llama3.2
        "gemma3:4b": ChatOllama(model="gemma3:4b", temperature=temp),
        "qwen3:latest": ChatOllama(model="qwen3:latest", temperature=temp)
    }

    rag_chains = {}
    meta_data_by_model = {} # ç”¨æ–¼å„²å­˜æ¯å€‹æ¨¡å‹çš„æ‘˜è¦ã€é—œéµå­—ã€å•é¡Œ

    for model_name, llm_instance in models_to_use.items():
        # ç‚ºæ¯å€‹æ¨¡å‹å‰µå»ºä¸€å€‹ RAG æç¤ºæ¨¡æ¿
        rag_prompt = ChatPromptTemplate.from_messages([
            ("system", f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©ç†ï¼Œç¾åœ¨ä½ ä½¿ç”¨çš„æ¨¡å‹æ˜¯ {model_name}ã€‚è«‹æ ¹æ“šä»¥ä¸‹æª¢ç´¢åˆ°çš„è³‡è¨Šä¾†å›ç­”å•é¡Œã€‚
              å¦‚æœè³‡è¨Šä¸­æ²’æœ‰æåˆ°ï¼Œè«‹ç¦®è²Œåœ°èªªä½ ç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚
              \n\næª¢ç´¢åˆ°çš„è³‡è¨Šï¼š\n{{context}}"""),
            ("human", "{question}")
        ])
        rag_chain_input_mapper = {"context": retriever, "question": RunnablePassthrough()}
        rag_chains[model_name] = rag_chain_input_mapper | rag_prompt | llm_instance | StrOutputParser()

        # ç‚ºæ¯å€‹æ¨¡å‹ç”Ÿæˆæ‘˜è¦ã€é—œéµå­—å’Œå•é¡Œ
        summarize_prompt = ChatPromptTemplate.from_messages([
            ("system", f"è«‹æ ¹æ“šä»¥ä¸‹æ–‡æœ¬å…§å®¹ï¼Œç¸½çµå‡ºä¸‰å€‹ä¸»è¦é‡é»ã€‚æ¯å€‹é‡é»è«‹ç”¨æ¢åˆ—å¼å‘ˆç¾ï¼Œä¸”ä¸è¶…é20å€‹å­—ã€‚è«‹ç”±æ¨¡å‹ {model_name} åŸ·è¡Œã€‚\n\næ–‡æœ¬å…§å®¹:\n{{text}}"),
            ("human", "è«‹ç¸½çµã€‚")
        ])
        summarize_chain = {"text": RunnablePassthrough()} | summarize_prompt | llm_instance | StrOutputParser()
        summary = summarize_chain.invoke(context_text)

        keywords_prompt = ChatPromptTemplate.from_messages([
            ("system", f"è«‹æ ¹æ“šä»¥ä¸‹æ–‡æœ¬å…§å®¹ï¼Œæå–å‡ºä¸‰å€‹æœ€é‡è¦çš„é—œéµå­—ã€‚è«‹ç”¨é€—è™Ÿåˆ†éš”ã€‚è«‹ç”±æ¨¡å‹ {model_name} åŸ·è¡Œã€‚\n\næ–‡æœ¬å…§å®¹:\n{{text}}"),
            ("human", "è«‹æå–é—œéµå­—ã€‚")
        ])
        keywords_chain = {"text": RunnablePassthrough()} | keywords_prompt | llm_instance | StrOutputParser()
        keywords = keywords_chain.invoke(context_text)

        questions_prompt = ChatPromptTemplate.from_messages([
            ("system", f"æ ¹æ“šä»¥ä¸‹æ–‡æœ¬å…§å®¹ï¼Œè¨­æƒ³ä½¿ç”¨è€…å¯èƒ½æœ€æ„Ÿèˆˆè¶£çš„ä¸‰å€‹å•é¡Œã€‚è«‹ç”¨æ¢åˆ—å¼å‘ˆç¾ã€‚è«‹ç”±æ¨¡å‹ {model_name} åŸ·è¡Œã€‚\n\næ–‡æœ¬å…§å®¹:\n{{text}}"),
            ("human", "è«‹æå‡ºå•é¡Œã€‚")
        ])
        questions_chain = {"text": RunnablePassthrough()} | questions_prompt | llm_instance | StrOutputParser()
        questions = questions_chain.invoke(context_text)

        meta_data_by_model[model_name] = {
            "summary": summary,
            "keywords": keywords,
            "questions": questions
        }

    return rag_chains, meta_data_by_model


def highlight_important_tokens(text, keywords):
    """
    ç”¨ç´…è‰²ç²—é«”æ¨™è¨˜æ–‡æœ¬ä¸­çš„é—œéµå­—ã€‚
    keywords æ‡‰è©²æ˜¯ä¸€å€‹åˆ—è¡¨æˆ–é›†åˆï¼ŒåŒ…å«è¦æ¨™è¨˜çš„é—œéµå­—ã€‚
    """
    highlighted_text = text
    # Split keywords string into a list
    keyword_list = [k.strip() for k in keywords.split(',') if k.strip()]

    for keyword in keyword_list:
        # Using a simple replace. For more robust highlighting (e.g., avoiding partial word matches),
        # you might need regex with word boundaries.
        highlighted_text = highlighted_text.replace(
            keyword, f'<span style="color:red;font-weight:bold;">{keyword}</span>'
        )
    return highlighted_text


# endregion

# region --- åˆå§‹è®Šæ•¸ ---
if "messages" not in st.session_state:  # æ­·å²è¨Šæ¯
    st.session_state.messages = []
if "rag_system_ready" not in st.session_state:  # ç•¶ç³»çµ±æº–å‚™å¥½çš„æ™‚å€™ï¼Œä¹Ÿå°±æ˜¯ä»£è¡¨ä½¿ç”¨è€…å·²ç¶“ç¢ºèªå°‡è³‡æ–™åšåˆå§‹åŒ–ï¼Œæ¥è‘—æ–¹èƒ½é–‹å•Ÿå•ç­”
    st.session_state.rag_system_ready = False
if "rag_chains" not in st.session_state:  # ragéŠæ ¸å¿ƒï¼Œæˆ‘å€‘æ˜¯åˆ©ç”¨ rag_chains[model_name].invoke() ä¾†å–å¾— ai è³‡è¨Š
    st.session_state.rag_chains = {}
if "initial_kb_loaded" not in st.session_state:  # è¿½è¹¤æ˜¯å¦å·²é¦–æ¬¡è¼‰å…¥çŸ¥è­˜åº«
    st.session_state.initial_kb_loaded = False
if "show_confirm_modal" not in st.session_state:  # æ˜¯å¦é¡¯ç¤ºå†ç¢ºèªæ–¹æ¡†
    st.session_state.show_confirm_modal = False
if "meta_data_by_model" not in st.session_state: # å„²å­˜æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•¸æ“š
    st.session_state.meta_data_by_model = {}
# endregion

# region --- åˆå§‹æŒ‰éˆ• ---
if st.button("åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«"):  # ç•¶ä½¿ç”¨è€…é»æ“Šæ­¤æŒ‰éˆ•æ™‚ï¼Œæ ¹æ“šç‹€æ…‹æ±ºå®šæ˜¯ç›´æ¥åˆå§‹åŒ–é‚„æ˜¯é¡¯ç¤ºç¢ºèªæ–¹æ¡†
    if not st.session_state.initial_kb_loaded:  # ç¬¬ä¸€æ¬¡é»æ“Šï¼šç›´æ¥åˆå§‹åŒ–çŸ¥è­˜åº«
        if len(final_context_text) == 0:
            st.error("è«‹åœ¨æ–‡å­—å€åŸŸä¸­è¼¸å…¥æ‚¨çš„è³‡è¨Šï¼Œæˆ–ä½¿ç”¨ç¯„ä¾‹æ–‡æœ¬ã€‚")
        else:
            with st.spinner("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹ï¼Œè«‹ç¨å€™..."):
                try:
                    rag_chains, meta_data_by_model = setup_rag_system_dynamic(final_context_text, temperature)
                    st.session_state.rag_chains = rag_chains
                    st.session_state.meta_data_by_model = meta_data_by_model # å„²å­˜æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•¸æ“š
                    st.session_state.rag_system_ready = True
                    st.session_state.initial_kb_loaded = True  # æ¨™è¨˜ç‚ºå·²è¼‰å…¥
                    st.success("ğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ç¾åœ¨å¯ä»¥é–‹å§‹æå•äº†ã€‚")
                    st.session_state.messages = []  # æ¸…ç©ºèŠå¤©æ­·å²
                except Exception as e:
                    st.session_state.rag_system_ready = False
                    st.session_state.rag_chains = {}
                    st.session_state.meta_data_by_model = {} # æ¸…ç©ºå…ƒæ•¸æ“š
                    st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ‰€éœ€æ¨¡å‹å·²ä¸‹è¼‰ã€‚")
                    st.warning("è«‹ç¢ºä¿æ‚¨å·²å®‰è£ Ollama ä¸¦å·²æ‹‰å–ä»¥ä¸‹æ‰€æœ‰æ¨¡å‹ï¼š`nomic-embed-text`ã€`llama3.2`ã€`gemma3:4b` å’Œ `qwen3:latest`ã€‚")
                    st.warning("â— **é‡è¦æç¤ºï¼š** å„˜ç®¡ `gemma3:4b` æ”¯æ´å¤šæ¨¡æ…‹ï¼Œä½†æ‚¨çš„æ‡‰ç”¨ç›®å‰åƒ…æ”¯æ´æ–‡æœ¬RAGã€‚")
    else:
        st.session_state.show_confirm_modal = True  # å¦‚æœå·²ç¶“åˆå§‹åŒ–éï¼Œå‰‡è·³åˆ° show_confirm_modal ç¢ºèªæ–¹æ¡†é‚è¼¯(ä¸‹æ–¹)
        st.rerun()  # å¼·åˆ¶ Streamlit é‡æ–°åŸ·è¡Œï¼Œä»¥ç«‹å³é¡¯ç¤ºç¢ºèªæ–¹æ¡†
# endregion

# region --- æ–¹æ¡†é‚è¼¯ ---
if st.session_state.show_confirm_modal:  # åªæœ‰ç•¶ st.session_state.show_confirm_modal ç‚º True æ™‚æ‰é¡¯ç¤º
    st.markdown("---")  # åˆ†éš”ç·š
    st.info("â„¹ï¸ æ‚¨å·²ç¶“åˆå§‹åŒ–éçŸ¥è­˜åº«ã€‚å†æ¬¡é»æ“Šè¡¨ç¤ºæ‚¨è¦**æ›´æ–°**çŸ¥è­˜åº«ã€‚")
    st.write("æ˜¯å¦ç¢ºå®šè¦ç”¨ç•¶å‰æ–‡å­—å€åŸŸçš„è³‡è¨Šä¾†æ›´æ–°çŸ¥è­˜åº«ï¼Ÿé€™å°‡æ¸…é™¤ç•¶å‰å°è©±æ­·å²ï¼Œä¸¦ä¸”ç¢ºèªæ‚¨æä¾›çš„è³‡è¨Š**æ˜¯å¦å‰å¾Œæœ‰ç‰´è§¸***ã€‚")
    col_confirm, col_cancel = st.columns(2)  # è¨­ç½®å…©å€‹æŒ‰éˆ•ï¼Œç”¨æ–¼ç¢ºèªæˆ–å–æ¶ˆæ“ä½œ
    with col_confirm:
        if st.button("ç¢ºå®šæ›´æ–°", key="confirm_update_kb_button"):  # é»æ“Šã€Œç¢ºå®šæ›´æ–°ã€æŒ‰éˆ•
            if len(final_context_text) == 0:  # åŸ·è¡ŒçŸ¥è­˜åº«æ›´æ–°çš„é‚è¼¯
                st.error("è«‹åœ¨æ–‡å­—å€åŸŸä¸­è¼¸å…¥æ‚¨çš„è³‡è¨Šï¼Œæˆ–ä½¿ç”¨ç¯„ä¾‹æ–‡æœ¬ã€‚")
                st.session_state.show_confirm_modal = False  # å‡ºéŒ¯æ™‚éš±è—æ–¹æ¡†
                st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
            else:
                with st.spinner("ğŸ”„ æ­£åœ¨æ›´æ–°çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹ï¼Œè«‹ç¨å€™..."):
                    try:
                        rag_chains, meta_data_by_model = setup_rag_system_dynamic(final_context_text, temperature)
                        st.session_state.rag_chains = rag_chains
                        st.session_state.meta_data_by_model = meta_data_by_model # å„²å­˜æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•¸æ“š
                        st.session_state.rag_system_ready = True
                        st.success("âœ… çŸ¥è­˜åº«å·²æˆåŠŸæ›´æ–°ï¼")
                        st.session_state.messages = []  # æ¸…ç©ºèŠå¤©æ­·å²
                        st.session_state.show_confirm_modal = False  # éš±è—ç¢ºèªæ–¹æ¡†
                        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
                    except Exception as e:
                        st.session_state.rag_system_ready = False
                        st.session_state.rag_chains = {}
                        st.session_state.meta_data_by_model = {} # æ¸…ç©ºå…ƒæ•¸æ“š
                        st.error(f"æ›´æ–°å¤±æ•—ï¼š{e}ã€‚è«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ‰€éœ€æ¨¡å‹å·²ä¸‹è¼‰ã€‚")
                        st.session_state.show_confirm_modal = False  # å‡ºéŒ¯æ™‚éš±è—æ–¹æ¡†
                        st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
    with col_cancel:
        if st.button("å–æ¶ˆ", key="cancel_update_kb_button"):  # é»æ“Šã€Œå–æ¶ˆã€æŒ‰éˆ•
            st.session_state.show_confirm_modal = False  # éš±è—ç¢ºèªæ–¹æ¡†
            st.rerun()  # å¼·åˆ¶é‡æ–°åŸ·è¡Œä»¥æ›´æ–°UI
# endregion

# region --- é¡¯ç¤ºæ‘˜è¦å’Œé—œéµå­— (åˆ†é–‹é¡¯ç¤ºæ¯å€‹æ¨¡å‹çš„çµæœ) ---
if st.session_state.rag_system_ready and st.session_state.meta_data_by_model:
    st.markdown("---")
    st.subheader("ğŸ’¡ çŸ¥è­˜åº«æ¦‚è¦½ (å„æ¨¡å‹è¦–è§’)")

    for model_name, data in st.session_state.meta_data_by_model.items():
        with st.expander(f"**æ¨¡å‹ï¼š{model_name} çš„æ¦‚è¦½**"):
            if data["summary"]:
                st.markdown("### ğŸ” **é‡é»æ‘˜è¦**")
                # é€™è£¡çš„ keywords æ˜¯ç•¶å‰æ¨¡å‹çš„ keywordsï¼Œè€Œéæ‰€æœ‰æ¨¡å‹çš„
                highlighted_summary = highlight_important_tokens(data["summary"], data["keywords"])
                st.markdown(highlighted_summary, unsafe_allow_html=True)

            if data["keywords"]:
                st.markdown("### ğŸ”‘ **é—œéµå­—**")
                highlighted_keywords = highlight_important_tokens(data["keywords"], data["keywords"])
                st.markdown(highlighted_keywords, unsafe_allow_html=True)

            if data["questions"]:
                st.markdown("### â“ **ä½¿ç”¨è€…å¯èƒ½å¥½å¥‡çš„å•é¡Œ**")
                highlighted_questions = highlight_important_tokens(data["questions"], data["keywords"])
                st.markdown(highlighted_questions, unsafe_allow_html=True)
    st.markdown("---")
# endregion

# region --- èŠå¤©ä»‹é¢é‚è¼¯ ---
# é¡¯ç¤ºæ‰€æœ‰æ­·å²è¨Šæ¯ï¼Œå€åˆ†ç”¨æˆ¶å’Œä¸åŒæ¨¡å‹çš„åŠ©ç†å›è¦†
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message("user"):
            st.markdown(message["content"])
    elif message["role"] == "assistant":
        with st.chat_message("assistant"):
            # å¦‚æœæ˜¯å¤šæ¨¡å‹å›è¦†ï¼Œå‰‡é¡¯ç¤ºæ¨¡å‹åç¨±
            if "model" in message:
                st.markdown(f"**ğŸ¤– {message['model']} çš„å›è¦†ï¼š**")
            st.markdown(message["content"])

if st.session_state.rag_system_ready:  # ç²å–ç”¨æˆ¶è¼¸å…¥ (åªæœ‰ç•¶ RAG ç³»çµ±å°±ç·’æ™‚æ‰å•Ÿç”¨è¼¸å…¥æ¡†)
    if not st.session_state.messages:  # åˆ¤æ–·æ˜¯å¦é¡¯ç¤ºé¦–æ¬¡å°è©±çš„åŠ©ç†æç¤ºè¨Šæ¯ (åƒ…åœ¨èŠå¤©æ­·å²ç‚ºç©ºä¸”åŠ©ç†å·²æº–å‚™å¥½æ™‚é¡¯ç¤º)
        initial_chat_message = "æ‚¨å¥½ï¼Œè«‹å•æœ‰ä»€éº¼æˆ‘å¯ä»¥å”åŠ©æ‚¨çš„ï¼Ÿ"
        with st.chat_message("assistant"):
            st.markdown(initial_chat_message)
            st.session_state.messages.append({"role": "assistant", "content": initial_chat_message})
    if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ..."):  # å°‡ç”¨æˆ¶è¨Šæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²ä¸¦é¡¯ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # è¿­ä»£æ‰€æœ‰ RAG éˆä¸¦ç²å–å›è¦†
        for model_name, rag_chain in st.session_state.rag_chains.items():
            with st.chat_message("assistant"):
                try:
                    st.markdown(f"**ğŸ¤– {model_name} æ­£åœ¨æ€è€ƒä¸­...**") # é¡¯ç¤ºæ¨¡å‹æ­£åœ¨è™•ç†çš„æç¤º
                    ai_response = rag_chain.invoke(prompt)  # èª¿ç”¨ RAG éˆç²å–å›è¦†ã€‚
                    st.markdown(f"**ğŸ¤– {model_name} çš„å›è¦†ï¼š**") # æœ€çµ‚é¡¯ç¤ºæ¨¡å‹åç¨±
                    st.markdown(ai_response)  # é¡¯ç¤ºæœ€çµ‚å›è¦†
                    # å°‡å›è¦†æ·»åŠ åˆ°èŠå¤©æ­·å²ï¼ŒåŒ…å«æ¨¡å‹åç¨±
                    st.session_state.messages.append({"role": "assistant", "model": model_name, "content": ai_response})
                except Exception as e:
                    error_message = f"å¾ˆæŠ±æ­‰ï¼Œ**{model_name}** è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}ã€‚\nè«‹ç¢ºèª Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸”æ¨¡å‹å·²ä¸‹è¼‰ã€‚"
                    st.error(error_message)  # å°‡éŒ¯èª¤è¨Šæ¯ä½œç‚º AI å›è¦†
                    # å°‡éŒ¯èª¤è¨Šæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²
                    st.session_state.messages.append({"role": "assistant", "model": model_name, "content": error_message})
else:  # å¦‚æœ RAG ç³»çµ±å°šæœªå°±ç·’ï¼Œé¡¯ç¤ºæç¤ºè¨Šæ¯ä¸¦ç¦ç”¨èŠå¤©è¼¸å…¥æ¡†
    st.warning("è«‹å…ˆè¼¸å…¥æ‚¨çš„è³‡è¨Šä¸¦é»æ“Šã€Œåˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«ã€æŒ‰éˆ•ä¾†å•Ÿå‹•åŠ©ç†ã€‚")
    st.chat_input("è«‹å…ˆåˆå§‹åŒ–çŸ¥è­˜åº«...", disabled=True)


# endregion

# region --- å´æ¬„èªªæ˜ ---
st.sidebar.markdown("---")
st.sidebar.header("ä½¿ç”¨èªªæ˜")
st.sidebar.markdown("""
é€™å€‹æ‡‰ç”¨ç¨‹å¼æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©ç†ï¼Œå®ƒæœƒ**å®Œå…¨æ ¹æ“šæ‚¨åœ¨æ–‡å­—å€åŸŸä¸­æä¾›çš„æœ€æ–°è³‡è¨Š**ä¾†å›ç­”å•é¡Œã€‚

**æ“ä½œæ­¥é©Ÿï¼š**
1.  åœ¨æ–‡å­—å€åŸŸä¸­**è¼¸å…¥æˆ–ä¿®æ”¹**æ‚¨çš„è³‡è¨Šã€‚
2.  **é‡è¦ï¼**
    * **ç¬¬ä¸€æ¬¡**é»æ“Šã€Œ**åˆå§‹åŒ–/æ›´æ–°çŸ¥è­˜åº«**ã€æŒ‰éˆ•ï¼Œå°‡æœƒè¼‰å…¥è³‡è¨Šä¸¦å•Ÿå‹•åŠ©ç†ã€‚
    * **å¾ŒçºŒ**é»æ“Šæ­¤æŒ‰éˆ•ï¼Œæœƒå½ˆå‡ºä¸€å€‹ç¢ºèªæ–¹æ¡†ï¼Œè©¢å•æ‚¨æ˜¯å¦è¦**æ›´æ–°**çŸ¥è­˜åº«ã€‚é»æ“Šã€Œç¢ºå®šæ›´æ–°ã€å°‡æœƒæ¸…é™¤èˆŠå°è©±ä¸¦ä½¿ç”¨æ–°è³‡è¨Šã€‚
3.  ç•¶çœ‹åˆ°ã€ŒğŸ‰ çŸ¥è­˜åº«èˆ‡ AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ã€æˆ–ã€Œâœ… çŸ¥è­˜åº«å·²æˆåŠŸæ›´æ–°ï¼ã€è¨Šæ¯å¾Œï¼Œæ‚¨å°±å¯ä»¥åœ¨ä¸‹æ–¹çš„èŠå¤©æ¡†ä¸­æå•äº†ã€‚

**é‡è¦æé†’ï¼š**
è«‹ç¢ºä¿æ‚¨çš„é›»è…¦å·²**å®‰è£ Ollama æœå‹™**ä¸¦å·²**ä¸‹è¼‰ä»¥ä¸‹æ‰€æœ‰æ¨¡å‹**ï¼š
-   **åµŒå…¥æ¨¡å‹ (Embedding Model):** `nomic-embed-text`
    åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull nomic-embed-text`
-   **èªè¨€æ¨¡å‹ (Language Model) çµ„åˆï¼š**
    * `llama3.2`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull llama3.2`
    * `gemma3:4b`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull gemma3:4b`
    * `qwen3:latest`
        åŸ·è¡Œå‘½ä»¤ï¼š`ollama pull qwen3:latest`

**é—œæ–¼æ¨¡å‹èƒ½åŠ›çš„æ³¨æ„äº‹é …ï¼š**
* `gemma3:4b` æ˜¯ä¸€å€‹æ”¯æ´**å¤šæ¨¡æ…‹ (åœ–åƒèˆ‡æ–‡æœ¬)** çš„å„ªç§€æ¨¡å‹ï¼ç„¶è€Œï¼Œåœ¨ç›®å‰é€™å€‹æ‡‰ç”¨ç¨‹å¼ä¸­ï¼Œ**æ‚¨çš„çŸ¥è­˜åº«æ˜¯ç´”æ–‡æœ¬ï¼Œä¸”ç¨‹å¼ç¢¼é‚è¼¯ä¹Ÿåªè™•ç†æ–‡å­—è¼¸å…¥**ã€‚è‹¥è¦å¯¦ç¾åœ–ç‰‡è¼¸å…¥å’ŒåŸºæ–¼åœ–ç‰‡çš„ RAGï¼Œéœ€è¦å°ç¨‹å¼ç¢¼é€²è¡Œé€²ä¸€æ­¥çš„é–‹ç™¼å’Œä¿®æ”¹ã€‚
* å°æ–¼çŸ¥è­˜åº«ä¸­æ²’æœ‰çš„è³‡è¨Šï¼ŒAI æœƒç¦®è²Œåœ°è¡¨ç¤ºç„¡æ³•å›ç­”ï¼Œä¸¦é¿å…ç·¨é€ å…§å®¹ã€‚
""")
# endregion